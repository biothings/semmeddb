{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9abcd0e-3cc3-4147-8f74-507869747368",
   "metadata": {},
   "source": [
    "## Extract Meta-Triples from Biothings SemMedDB API's Predication Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c047f2d-be8b-48ec-b621-f2d332b02cb1",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Reference: https://github.com/biothings/pending.api/issues/63#issuecomment-1100469563\n",
    "\n",
    "A meta-triple is a unit of `(SUBJECT_SEMTYPE, PREDICATE, OBJECT_SEMTYPE)` values (see table below). E.g. `(dsyn, PROCESS_OF, humn)`, which can be roughly interpreted as \"Disease or SYNdrome is a PROCESS OF HUMaN\". Statistics on such meta-triples can help determine related x-bte annotations. Later we find that the entity types of subjects/objects are also helpful to x-bte developers, so two more fields, `SUBJECT_PREFIX` and `OBJECT_PREFIX`, are also included in the statistics. However we still call it a meta-triple.\n",
    "\n",
    "The source file (`semmedVER43_2023_R_PREDICATION.116080_clean_pyarrow_snappy.parquet`) we use here is a cleaner and wangled version of the original SemMedDB predications (`semmedVER43_2023_R_PREDICATION.116080`), and it's stored in [Apache Parquet](https://parquet.apache.org/) format for better I/O performance. It can be found on server `su06` under directory `/data/pending/datasources/semmeddb/43/CACHE`.\n",
    "\n",
    "The columns of the source file are listed below:\n",
    "\n",
    "|Column Name     |Remark                                                        |\n",
    "|----------------|--------------------------------------------------------------|\n",
    "|`_ID`           | ID of the document parsed from this row                      |\n",
    "|`PREDICATION_ID` | ID of the predication                                        |\n",
    "|`PREDICATE`     | Predicate of the predication                                 |\n",
    "|`PMID`          | PubMed ID of the predication                                 |\n",
    "|`SUBJECT_CUI`   | Subject's CUI (either UMLS CUI or NCBIGene ID)               |\n",
    "|`SUBJECT_PREFIX` | Indicator of Subject's CUI type (either `\"umls\"` or `\"ncbigene\"`)|\n",
    "|`SUBJECT_NAME`  | Subject's name                                               |\n",
    "|`SUBJECT_SEMTYPE` | Subject's semantic type (4-letter abbreviation)              |\n",
    "|`SUBJECT_NOVELTY` | Subject's novelty score (alway 1 currently)                  |\n",
    "|`OBJECT_CUI`    | Object's CUI (either UMLS CUI or NCBIGene ID)                |\n",
    "|`OBJECT_PREFIX` | Indicator of Object's CUI type (either `\"umls\"` or `\"ncbigene\"`)|\n",
    "|`OBJECT_NAME`   | Object's name                                                |\n",
    "|`OBJECT_SEMTYPE` | Object's semantic type (4-letter abbreviation)               |\n",
    "|`OBJECT_NOVELTY` | Object's novelty score (alway 1 currently)                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e2dfc-4d72-4280-9eca-12399410fc74",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "Apache Arrow is an ideal in-memory transport layer for data that is being read or written with Parquet files. To load the parquet file, we need to install `pyarrow` package. E.g. with `pip`:\n",
    "\n",
    "```bash\n",
    "pip install pyarrow\n",
    "```\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Installing PyArrow](https://arrow.apache.org/docs/python/install.html)\n",
    "- [Reading and Writing the Apache Parquet Format](https://arrow.apache.org/docs/python/parquet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661529e0-8f6b-474f-8584-ac973672c623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "parquet_file = \"semmedVER43_2024_R_PREDICATION_clean_pyarrow_snappy.parquet\"\n",
    "\n",
    "semmed_df = pd.read_parquet(\n",
    "    parquet_file,\n",
    "    engine=\"pyarrow\",\n",
    "    columns=[\n",
    "        \"SUBJECT_PREFIX\",\n",
    "        \"SUBJECT_SEMTYPE\",\n",
    "        \"PREDICATE\",\n",
    "        \"OBJECT_PREFIX\",\n",
    "        \"OBJECT_SEMTYPE\",\n",
    "        \"PMID\",\n",
    "        \"SUBJECT_CUI\",\n",
    "        \"OBJECT_CUI\"\n",
    "    ]\n",
    ")\n",
    "semmed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5536d3-a6d6-44f6-8310-b674e4d63890",
   "metadata": {},
   "outputs": [],
   "source": [
    "semmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fa67e-42a8-45a1-95d0-4375418bcf10",
   "metadata": {},
   "source": [
    "### Meta-Triples vs No. Predications\n",
    "\n",
    "Because each row is a predication, we can do `.value_counts()` for each combination of `SUBJECT_PREFIX`, `SUBJECT_SEMTYPE`, `PREDICATE`, `OBJECT_PREFIX`, and `OBJECT_SEMTYPE`, for the No. predications of each meta-triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b7de4-2d55-44d3-8004-77ed472c0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stat = semmed_df.value_counts(subset=[\"SUBJECT_PREFIX\", \"SUBJECT_SEMTYPE\", \"PREDICATE\", \"OBJECT_PREFIX\", \"OBJECT_SEMTYPE\"]).reset_index(name=\"PREDICATION_N\")\n",
    "pred_stat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d8f38-d1bd-4365-a0e9-496d53028ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the top 10 meta-triples\n",
    "pred_stat.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ca2f3-4c8a-40bc-9e30-40f8d50a4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the top 10 meta-triples whose subject is a NCBIGene\n",
    "pred_stat.loc[pred_stat[\"SUBJECT_PREFIX\"].eq(\"ncbigene\")].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83b722-7cb7-4d1e-a409-928f5e2ea6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Result\n",
    "pred_stat.to_csv(\"meta_triple_predication_stat.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea38a68-5485-4e7a-91b3-3f9015e979b8",
   "metadata": {},
   "source": [
    "### Meta-Triples vs No. Documents (Original Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dcb61-d303-46dc-8c74-793e58bbe3ba",
   "metadata": {},
   "source": [
    "Each semmeddb document has an ID made from 3 fields, `SUBJECT_CUI`, `PREDICATE`, and `OBJECT_CUI`. Therefore we group the input data frame by these 3 fields, and each group should contribute only 1 document to the stats. However on BTE ends, documents with `PMID` counts less than or equal 3 are often excluded due to their low significance. We also take the valid contribution of documents into account in making the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3c82bc-106e-417e-bad0-e48c2d4fb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = semmed_df.groupby([\"SUBJECT_CUI\", \"PREDICATE\", \"OBJECT_CUI\"])\n",
    "\n",
    "# Count the number of unique PMIDs for each group\n",
    "doc_stat = g[\"PMID\"].nunique().reset_index(name=\"PMID_N_UNIQUE\")\n",
    "\n",
    "# Each group should contribute only 1 document\n",
    "doc_stat[\"DOC_N\"] = 1\n",
    "\n",
    "# If a group has less than or equal 3 PMIDs, it's valid contribution is 0\n",
    "doc_stat[\"DOC_N_VALID\"] = doc_stat[\"DOC_N\"].where(doc_stat[\"PMID_N_UNIQUE\"] > 3, other=0)\n",
    "\n",
    "doc_stat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473c038-634b-4d74-8e89-6c8f7062a7ac",
   "metadata": {},
   "source": [
    "Now join the document contribution stats to the orignal data frame. Note that a subject/object may have multiple semtypes. E.g. in [C0009325-INHIBITS-C0162574](https://biothings.transltr.io/semmeddb/association/C0009325-INHIBITS-C0162574), the subject has two semtypes, `aapp` and `phsu`, and it's mapped to two meta-triples, `(aapp, INHIBITS, bacs)` and `(phsu, NHIBITS, bacs)`. In such cases, all the meta-triples will receive the contribution of documents from the same ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924b0d6-0ccd-4c16-b088-e773b6fa6f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Can we use transform for all the 3 new columns, without merging? E.g. semmed_df[\"PMID_N_UNIQUE\"] = g[\"PMID\"].transform(\"nunique\")\n",
    "semmed_df = semmed_df.merge(doc_stat, how=\"inner\", on=[\"SUBJECT_CUI\", \"PREDICATE\", \"OBJECT_CUI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b1a31-10ff-4112-913d-63ad1f92fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the meta-triples\n",
    "g = semmed_df.groupby([\"SUBJECT_PREFIX\", \"SUBJECT_SEMTYPE\", \"PREDICATE\", \"OBJECT_PREFIX\", \"OBJECT_SEMTYPE\"])\n",
    "\n",
    "\"\"\"\n",
    "Here I encountered a bug in Pandas.GroupBy, whose root cause might be in the pyarrow implementation of GroupBy. \n",
    "\n",
    "If I call `g[\"DOC_N\"].sum()`, it will exhaust the Cartesian product of all the values from the five columns, \n",
    "    \"SUBJECT_PREFIX\", \"SUBJECT_SEMTYPE\", \"PREDICATE\", \"OBJECT_PREFIX\", and \"OBJECT_SEMTYPE\", leading to numerous empty groups with sum of \"DOC_N\" equal 0,\n",
    "    which is quite counter-intuitive because each group's DOC_N shoule be at least 1.\n",
    "    \n",
    "Therefore I take the following workaroud to make the aggregation manually.\n",
    "\"\"\"\n",
    "doc_stat2 = [[*index, data[\"DOC_N\"].sum(), data[\"DOC_N_VALID\"].sum()] for index, data in g]\n",
    "doc_stat2 = pd.DataFrame(doc_stat2, columns=[\"SUBJECT_PREFIX\", \"SUBJECT_SEMTYPE\", \"PREDICATE\", \"OBJECT_PREFIX\", \"OBJECT_SEMTYPE\", \"DOC_N\", \"DOC_N_VALID\"])\n",
    "doc_stat2.sort_values(\"DOC_N_VALID\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a623107-2dbf-40ff-a89f-08372a082750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Result\n",
    "doc_stat2.to_csv(\"meta_triple_document_stat.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f60ae5",
   "metadata": {},
   "source": [
    "### Meta-Triples vs No. Documents (New Solution)\n",
    "******************** Alternative to merge ********************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba366051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Temporarily switch to a safer backend to avoid pyarrow groupby issues\n",
    "os.environ[\"PANDAS_DATAFRAME_BACKEND\"] = \"numpy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf00c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute document contribution stats directly with transform (no merge needed)\n",
    "group_cols_doc = [\"SUBJECT_CUI\", \"PREDICATE\", \"OBJECT_CUI\"]\n",
    "g_doc = semmed_df.groupby(group_cols_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780f6e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transform to get unique PMIDs and set DOC_N/DOC_N_VALID\n",
    "semmed_df[\"PMID_N_UNIQUE\"] = g_doc[\"PMID\"].transform(\"nunique\")\n",
    "semmed_df[\"DOC_N\"] = 1\n",
    "semmed_df[\"DOC_N_VALID\"] = semmed_df[\"PMID_N_UNIQUE\"].gt(3).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semmed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Group by meta-triples\n",
    "group_cols_meta = [\"SUBJECT_PREFIX\", \"SUBJECT_SEMTYPE\", \"PREDICATE\", \"OBJECT_PREFIX\", \"OBJECT_SEMTYPE\"]\n",
    "g_meta = semmed_df.groupby(group_cols_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add progress bar to manual group aggregation (loop workaround due to backend issues)\n",
    "print(\"[INFO] Aggregating meta-triple document statistics...\")\n",
    "\n",
    "# Using tqdm to track the progress of the loop\n",
    "meta_rows = []\n",
    "for index, data in tqdm(g_meta, total=len(g_meta), desc=\"Aggregating groups\"):\n",
    "    doc_n = data[\"DOC_N\"].sum()\n",
    "    doc_n_valid = data[\"DOC_N_VALID\"].sum()\n",
    "    meta_rows.append([*index, doc_n, doc_n_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d73cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataframe\n",
    "doc_stat2 = pd.DataFrame(meta_rows, columns=group_cols_meta + [\"DOC_N\", \"DOC_N_VALID\"])\n",
    "doc_stat2.sort_values(\"DOC_N_VALID\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddce5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the output file path with the date and time\n",
    "outfile = \"metatriple_output_files/meta_triple_document_stat.tsv\"\n",
    "# Save the Result\n",
    "doc_stat2.to_csv(outfile, sep=\"\\t\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
